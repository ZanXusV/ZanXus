---
layout:     post
title:      "Scrapy With Python3"
subtitle:   "Crawling  with scrapy"
date:       2016-08-03 15:40:00+0800
author:     "ZanXus"
header-img: "img/blog/header/post-bg-04.jpg"
thumbnail: /img/blog/thumbs/thumb04.png
tags: [scrapy]
category: [python3]
comments: true
share: true
---


# About  [Scrapy](http://scrapy.org/).

## **_Scrapy_**{: style="color: orange"} [Installation](http://scrapy.org/download/).

## **_Scrapy_**{: style="color: orange"} [Documentation](http://doc.scrapy.org/en/1.1/).

## The spider is very simple as follows.

> ### items.py

~~~ python
import scrapy

class Python3ScrapyItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass

class UnsplashItem(scrapy.Item):
    image_urls = scrapy.Field()
    images = scrapy.Field()
    raw = scrapy.Field()
    full = scrapy.Field()
    regular = scrapy.Field()
    small = scrapy.Field()
    thumb = scrapy.Field()
    pass
~~~

> ### unsplash_spider.py

~~~ python
import scrapy
import json
from Python3_Scrapy.items import UnsplashItem


class UnsplashSpider(scrapy.Spider):
    name = "unsplash"
    allowed_domains = ["unsplash.com"]
    urls = [
        "https://unsplash.com/napi/photos/curated?page=%d&per_page=12&order_by=latest" % n for n in range(1, 93)
        ]
    #If special request needed then override start_requests and remove start_urls,here i use it to add headers.
    #See docs in  http://doc.scrapy.org/en/latest/topics/spiders.html
    def start_requests(self):
        for url in self.urls:
            yield scrapy.Request(url, callback=self.parse_data, headers={
                'Authorization': 'Client-ID d69927c7ea5c770fa2ce9a2f1e3589bd896454f7068f689d8e41a25b54fa6042'}
                                 )

    #Parse response data,get wanted image urls and download them with image pipeline
    def parse_data(self, response):
        jsondata = response.body.decode('utf-8')
        filename = 'unsplash.json'
        with open(filename, 'a') as f:
            f.write(jsondata)

        data = json.loads(jsondata)
        for objects in data:
            urls = objects['urls']
            yield UnsplashItem(image_urls=[urls['full']], raw=urls['raw'], full=urls['full'],
                               regular=urls['regular'], small=urls['small'], thumb=urls['thumb'])

~~~

---

> ### settings.py. 

~~~ python
# ITEM_PIPELINES is necessary for downloading file or image.
ITEM_PIPELINES = {
    'scrapy.pipelines.images.ImagesPipeline': 1,
}
#path to store downloaded images
IMAGES_STORE = '/home/zanxus/develop/crawlled_pictures'
#set downloader timeout to 300 seconds
DOWNLOAD_TIMEOUT = 300
#if same imageurl crawled within 30 days then ignore it
IMAGES_EXPIRES = 30
~~~


### Complete code [https://github.com/ZanXusV/python3.scrapy](https://github.com/ZanXusV/python3.scrapy)
