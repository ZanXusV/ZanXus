---
layout:     post
title:      "Scrapy With Python3"
subtitle:   "Crawling WebPage Using Python3"
date:       2016-08-03 15:40:00
author:     "ZanXus"
header-img: "img/blog/header/post-bg-01.jpg"
thumbnail: /img/blog/thumbs/thumb01.png
tags: [scrapy]
category: [python3]
comments: true
share: true
---

# This is an usage using **_scrapy_**{: style="color: orange"} crawl images from [unsplash](https://unsplash.com/).

## After last try crawling,I was advised to use **_scrapy_**{: style="color: orange"} which is a python spider framework to crawl data.So I began to learn this framework.

## About  [Scrapy](http://scrapy.org/).

## **_scrapy_**{: style="color: orange"} [Installation](http://scrapy.org/download/).I have installed python2.7 and python3.5 on my ubuntu 16.04.However I encountered some problems while installing it with command line,my pip3 version is 8.1.1 and can't upgrade to 8.1.2,and every time i install scrapy using  command 'pip3 install scrapy'  hit errors.Finally i downloaded the **_scrapy_**{: style="color: orange"} zip archive and unzip it and then i run the setup.py,finally I install it smoothly at the first time.WTF!

## Scrapy [Documentation](http://doc.scrapy.org/en/1.1/).

## The spider is very simple as follows.

> ### items.py

~~~ python
import scrapy

class Python3ScrapyItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass

class UnsplashItem(scrapy.Item):
    image_urls = scrapy.Field()
    images = scrapy.Field()
    raw = scrapy.Field()
    full = scrapy.Field()
    regular = scrapy.Field()
    small = scrapy.Field()
    thumb = scrapy.Field()
    pass
~~~

> ### unsplash_spider.py

~~~ python
import scrapy
import json
from Python3_Scrapy.items import UnsplashItem


class UnsplashSpider(scrapy.Spider):
    name = "unsplash"
    allowed_domains = ["unsplash.com"]
    urls = [
        "https://unsplash.com/napi/photos/curated?page=%d&per_page=12&order_by=latest" % n for n in range(1, 93)
        ]
    #If special request needed then override start_requests and remove start_urls,here i use it to add headers.
    #See docs in  http://doc.scrapy.org/en/latest/topics/spiders.html
    def start_requests(self):
        for url in self.urls:
            yield scrapy.Request(url, callback=self.parse_data, headers={
                'Authorization': 'Client-ID d69927c7ea5c770fa2ce9a2f1e3589bd896454f7068f689d8e41a25b54fa6042'}
                                 )

    #Parse response data,get wanted image urls and download them with image pipeline
    def parse_data(self, response):
        jsondata = response.body.decode('utf-8')
        filename = 'unsplash.json'
        with open(filename, 'a') as f:
            f.write(jsondata)

        data = json.loads(jsondata)
        for objects in data:
            urls = objects['urls']
            yield UnsplashItem(image_urls=[urls['full']], raw=urls['raw'], full=urls['full'],
                               regular=urls['regular'], small=urls['small'], thumb=urls['thumb'])

~~~

---

> ### settings.py. Make settings as you need.

~~~ python
# ITEM_PIPELINES is necessary for downloading file or image.
ITEM_PIPELINES = {
    'scrapy.pipelines.images.ImagesPipeline': 1,
}
#path to store downloaded images
IMAGES_STORE = '/home/zanxus/develop/crawlled_pictures'
#set downloader timeout to 300 seconds
DOWNLOAD_TIMEOUT = 300
#if same imageurl crawled within 30 days then ignore it
IMAGES_EXPIRES = 30
~~~

## Conclusion: There are some points to do for further extension,such as store data into database and crawling target urls with auto search instead of current way.

